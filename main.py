import nltk
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, sent_tokenize
import string

nltk.download('punkt_tab')
nltk.download('vader_lexicon')

file_path = "text.txt"
try:
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
except FileNotFoundError:
    print(f"Ğ¤Ğ°Ğ¹Ğ» '{file_path}' Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾. ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ Ğ½Ğ°Ğ·Ğ²Ñƒ Ğ°Ğ±Ğ¾ ÑˆĞ»ÑÑ….")
    exit()


words = word_tokenize(text.lower())
words_clean = [word for word in words if word.isalpha()]
sentences = sent_tokenize(text)

num_words = len(words_clean)
num_sentences = len(sentences)
num_letters = sum(len(word) for word in words_clean)

print(f"ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ ÑĞ»Ñ–Ğ²: {num_words}")
print(f"ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ñ€ĞµÑ‡ĞµĞ½ÑŒ: {num_sentences}")
print(f"ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ»Ñ–Ñ‚ĞµÑ€: {num_letters}")


word_freq = Counter(words_clean)
most_common = word_freq.most_common(10)
print("\nĞĞ°Ğ¹Ñ‡Ğ°ÑÑ‚Ñ–ÑˆÑ– ÑĞ»Ğ¾Ğ²Ğ°:")
for word, freq in most_common:
    print(f"{word}: {freq}")

wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)
plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title('Ğ¥Ğ¼Ğ°Ñ€Ğ° ÑĞ»Ñ–Ğ²')
plt.show()

top_words = dict(most_common)
plt.figure(figsize=(10, 5))
plt.bar(top_words.keys(), top_words.values())
plt.title('ĞĞ°Ğ¹Ñ‡Ğ°ÑÑ‚Ñ–ÑˆÑ– ÑĞ»Ğ¾Ğ²Ğ°')
plt.xlabel('Ğ¡Ğ»Ğ¾Ğ²Ğ¾')
plt.ylabel('ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ')
plt.show()

sia = SentimentIntensityAnalyzer()
sentiment = sia.polarity_scores(text)

print("\nĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ñ Ñ‚ĞµĞºÑÑ‚Ñƒ:")
for k, v in sentiment.items():
    print(f"{k}: {v:.3f}")

compound = sentiment["compound"]

if compound >= 0.05:
    final_sentiment = "Positive ğŸ˜Š"
elif compound <= -0.05:
    final_sentiment = "Negative ğŸ˜ "
else:
    final_sentiment = "Neutral ğŸ˜"

print(f"\nĞ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ° Ğ¾Ñ†Ñ–Ğ½ĞºĞ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ñ: {final_sentiment}")
